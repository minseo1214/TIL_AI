<h2>비용함수</h2>
로지스틱 회귀의 가설이 <img alt="image" src="https://user-images.githubusercontent.com/68543486/133557539-a0b4bc08-6ace-4fd6-a90e-f1d6ad10b570.png">이다.
여기에서 최적의 W와 b를 찾을 수 있는 cost function을 정의해야한다.<br>
앞서 선형회귀에서 배운 비용함수인 편균 제곱오차를 로지스틱회귀의 비용함수로 그냥 사용하면 안될까?<br>
다은은 선형 회귀엥서 사용했던 평균제곱 오차의 수식이다.<br>
<img alt="image" src="https://user-images.githubusercontent.com/68543486/133557868-a946841b-2752-408a-8caf-38a0ed5a872f.png"><br>
그러면, <img alt="image" src="https://user-images.githubusercontent.com/68543486/133558073-1f9aedd0-7863-4a34-92ba-b5e669383f39.png">이러한 가정에서
<img alt="image" src="https://user-images.githubusercontent.com/68543486/133558160-9f6267e2-89fd-4ed8-a069-fc10ee4ed2d4.png">로 바뀌게 된다. 그리고 이 비용함수를 미분하면
선형 회귀때와 달리 비볼록(non-convex) 형태의 그래프가 나온다.<br>
<img alt="image" src="https://user-images.githubusercontent.com/68543486/133558317-3e251159-45f1-47be-bdf1-6013422ba1b1.png">
이것이 문제가 되는 이유는 경사하강법이 오차가 최소값이 되는 구간에 도착했다고 판단한 그 구간이 실제 오차가 완전히 최소값이 되는 구간이 아닐 수 있다는 점이다.<br>
만약, 실제 최소가 되는 구간을 잘못 판단하면 최적의 가중치W가 아닌 다른 값을 택해 모델의 성능이 더 오르지 않는다<br>
<br><br><br>

그렇다면 시그모이드 함수의 특징을 보자.<br>
시그모이드의 특징은 함수의 출력값이 0과 1 사이의 값이라는 점이다. 즉, 실제 값이 1일 때 예측값이 0이면 오차값이 커져야하고 실제 값이 0이고 예측값이 1이면 오차값이 커져야한다.<br>
바로 그 함수가 로그함수이다.<br>
<img alt="image" src="https://user-images.githubusercontent.com/68543486/133559349-ff89f048-57ff-4561-8585-fbb0d1666dca.png">
<br>
실제값이 1일 때의 그래프를 주황색 선으로 표현하였으며, 실제값이 0일 때의 그래프를 초록색 선으로 표현하였다.<br>
실제값이 1이라고 해보면 이 경우, 예측값인 H(x)의 값이 1이면 오차가 0이므로 당연히 cost는 0이 된다.<br>
반면, H(x)가 0으로 수렴하면 cost는 무한대로 발산한다.<br>
실제값이 0인 경우는 그 반대로 이해하면 된다.<br>
이 두 개의 로그 함수를 식으로 표현하면 다음과 같다.<br>
<img alt="image" src="https://user-images.githubusercontent.com/68543486/133559701-aedc37b5-b014-4d84-b735-69062cf0d099.png"><br>
<br>
그리고 그 식을 합치면
<img alt="image" src="https://user-images.githubusercontent.com/68543486/133562681-ceda88b2-7f87-4921-bae7-57295d0f847a.png">
이렇게 된다.
